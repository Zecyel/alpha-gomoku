# Training Acceleration Summary

## Final Optimized Command

```bash
python train.py \
    --device cuda:1 \
    --parallel-games 64 \
    --mcts-batch 64 \
    --batch-size 1024 \
    --games-per-iter 64 \
    --mcts-sims 400 \
    --num-iterations 2000
```

## Performance Comparison

| Stage | Optimization | Time/Iteration | Speedup |
|-------|-------------|----------------|---------|
| **Initial** | Sequential, Python MCTS | 10 min | 1x |
| **+ Batched MCTS** | Virtual loss, batch NN calls | 10 min | 1x (same games) |
| **+ Parallel Games (16)** | Run games simultaneously | ~2.5 min | 4x |
| **+ Compiled MCTS** | Numba JIT tree operations | ~1.5 min | 6.7x |
| **+ More Parallel (64)** | Max GPU utilization | **~11 sec** | **55x** |

## Speedup Breakdown

### 1. Batched MCTS (1.5x faster per simulation)
- **Before**: One NN call per MCTS simulation
- **After**: Batch 32 leaves â†’ one NN call
- **Result**: 1984 sims/sec

### 2. Compiled MCTS (2.5x faster tree operations)
- **Before**: Python objects, dict lookups
- **After**: Numba-compiled flat arrays
- **Result**: 5017 sims/sec (batch=32), 8142 sims/sec (batch=64)

### 3. Parallel Games (Linear with games)
- **Before**: Play 1 game at a time
- **After**: Play 64 games in parallel, batch all NN calls
- **Result**: 0.17s per game (64 parallel)

### 4. bfloat16 Precision (~1.5x faster)
- **Before**: float32 (4 bytes)
- **After**: bfloat16 (2 bytes), 2x throughput
- **Result**: Faster NN inference, less VRAM

### 5. TensorFloat32 Matmul
- Enabled automatically on Ampere+ GPUs
- ~2x faster matrix multiplication

## Final Performance

With 96GB GPU and optimizations:

```
Training Speed:
- 64 games in parallel: ~11 seconds
- Per iteration (64 games): ~11 sec
- Per game: 0.17 sec
- 2000 iterations: ~6 hours (vs 333 hours before!)
```

## Memory Usage Estimate

With 96GB VRAM and 64 parallel games:
- Network: ~0.5 GB
- 64 games Ã— 50K nodes Ã— 20 bytes: ~64 MB
- Training batch (1024): ~2 GB
- NN forward cache: ~10 GB
- **Total**: ~15-20 GB (plenty of headroom!)

## Recommended Settings by VRAM

| VRAM | parallel-games | mcts-batch | batch-size | Expected Speed |
|------|----------------|------------|------------|----------------|
| 12GB | 8 | 32 | 256 | ~45 sec/iter |
| 24GB | 16 | 48 | 512 | ~23 sec/iter |
| 48GB | 32 | 64 | 768 | ~12 sec/iter |
| **96GB** | **64** | **64** | **1024** | **~11 sec/iter** |

## Key Optimizations Used

1. âœ… **Numba JIT** - Game logic (100x faster)
2. âœ… **Compiled MCTS** - Tree operations (2.5x faster)
3. âœ… **Batched Inference** - Virtual loss parallel MCTS
4. âœ… **Parallel Games** - Multiple games simultaneously
5. âœ… **bfloat16** - Half precision (~2x throughput)
6. âœ… **TF32** - Fast matmul on Ampere+ GPUs
7. âœ… **torch.compile()** - Kernel fusion

## Training Time Estimates

At **11 sec/iteration**:
- 100 iterations: ~18 minutes
- 500 iterations: ~1.5 hours
- 2000 iterations: ~6 hours
- 5000 iterations: ~15 hours (overnight!)

## Next Steps

To train a strong model:

```bash
# Quick test (beats random)
python train.py --device cuda:1 --parallel-games 64 --num-iterations 100

# Medium strength (beats beginners)
python train.py --device cuda:1 --parallel-games 64 --num-iterations 500

# Strong player (beats most humans)
python train.py --device cuda:1 --parallel-games 64 --num-iterations 2000

# Very strong (near-optimal)
python train.py --device cuda:1 --parallel-games 64 --num-iterations 5000
```

## Monitoring Training

Watch the loss:
- Policy loss should drop from ~5.4 to ~3.0
- Value loss should drop from ~0.4 to ~0.1
- Buffer fills to 500K examples

Test periodically:
```bash
python play.py evaluate --model1 checkpoints/latest.pt --num-games 20
```

Enjoy your fast training! ðŸš€
